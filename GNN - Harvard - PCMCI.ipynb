{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db20886",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7be9bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Define folder path and TSV file containing labels\n",
    "participants_xlsx_file = '.\\\\data\\\\participants.xsls'\n",
    "participants_tsv_file = '.\\\\data\\\\participants.tsv'\n",
    "\n",
    "# Load TSV file\n",
    "df_labels = pd.read_csv(participants_tsv_file, sep='\\t')\n",
    "\n",
    "# Filter out rows where 'group' is 'n/a'\n",
    "df_labels = df_labels[df_labels['group'].notna() & (df_labels['group'] != 'n/a')]\n",
    "\n",
    "# Extract labels as a dictionary and convert to integers\n",
    "labels_dict = dict(zip(df_labels['participant_id'], df_labels['group'].astype(int)))\n",
    "\n",
    "# Function to load data\n",
    "def load_data(node2vec_folder, graph_shapes_file, csv_prefix, csv_suffix, gml_prefix, gml_suffix):\n",
    "    data_list = []  # List to store loaded data\n",
    "    missing_files = []  # List to track missing files\n",
    "    processed_subs = []  # List to track processed subjects\n",
    "\n",
    "    for sub_id, label in labels_dict.items():\n",
    "        # Correct the format of the subject ID to match the file format\n",
    "        sub_id_corrected = sub_id.replace('sub-', 'Sub')\n",
    "        csv_file = os.path.join(node2vec_folder, f\"Node2Vec_{csv_prefix}_{sub_id_corrected}_{csv_suffix}.csv\")\n",
    "        gml_file = os.path.join(graph_shapes_file, f\"GNN input_{gml_prefix}_{sub_id_corrected}_{gml_suffix}.gml\")\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(csv_file) and os.path.exists(gml_file):\n",
    "            # Load node features from CSV\n",
    "            node_features = pd.read_csv(csv_file, header=0, index_col=0).values\n",
    "            num_nodes, num_features = node_features.shape\n",
    "\n",
    "            # Load the graph structure from GML file\n",
    "            nx_graph = nx.read_gml(gml_file)\n",
    "            data = from_networkx(nx_graph)\n",
    "            edge_index = data.edge_index\n",
    "\n",
    "            # Create a complete graph for edge_index\n",
    "            # edge_index = torch.combinations(torch.arange(num_nodes), 2).t()\n",
    "            \n",
    "            # Convert data to PyTorch tensors\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "            y = torch.tensor([label - 1], dtype=torch.long)  # Convert labels to 0 and 1\n",
    "            \n",
    "            # Create a Data object for PyTorch Geometric\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "            processed_subs.append(sub_id)\n",
    "        else:\n",
    "            missing_files.append(sub_id)  # Add missing subject ID to the list\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd325b",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "db69403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for early stopping\n",
    "def early_stopping(val_accuracies, patience):\n",
    "    if len(val_accuracies) > patience:\n",
    "        recent_accuracies = val_accuracies[-patience:]\n",
    "        if max(recent_accuracies) <= val_accuracies[-patience-1]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e2fd6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3914a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter search with Grid Search\n",
    "param_grid = {\n",
    "    'hidden_channels': [16, 32, 64],\n",
    "    'dropout': [0.2, 0.4],\n",
    "    'lr': [0.01, 0.0001],\n",
    "    'weight_decay': [1e-2, 1e-4]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94cd7f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "84dabfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, mean_squared_error, precision_recall_curve, auc as sklearn_auc\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(labels, preds_binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds_binary, average='binary', zero_division=0)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds_binary)\n",
    "    average_precision = average_precision_score(labels, preds)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"conf_matrix\": conf_matrix,\n",
    "        \"ap\": average_precision,\n",
    "        \"mse\": mse,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            res = compute_metrics(out, data.y)\n",
    "            \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "646ecd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "def save_results(file_name, test_metrics, train_metrics, val_metrics, params):\n",
    "    \"\"\"\n",
    "    Save cross-validation train, validation, and test metrics along with hyperparameters \n",
    "    into a single Excel file with separate sheets for train, validation, and test results.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: Name of the file (without extension).\n",
    "    - train_metrics: List of train metrics for each fold.\n",
    "    - val_metrics: List of validation metrics for each fold.\n",
    "    - test_metrics: Dictionary of test metrics.\n",
    "    - params: Dictionary of hyperparameters.\n",
    "    \"\"\"\n",
    "    def reorder_columns(df):\n",
    "        \"\"\"Ensure the 'Fold' column is the first column.\"\"\"\n",
    "        columns = [\"Fold\"] + [col for col in df.columns if col != \"Fold\"]\n",
    "        return df[columns]\n",
    "\n",
    "    def create_df_with_mean(metrics_list, params):\n",
    "        # Create a DataFrame for metrics\n",
    "        df = pd.DataFrame(metrics_list)\n",
    "        df[\"Fold\"] = range(1, len(metrics_list) + 1)  # Add fold numbers\n",
    "        for key, value in params.items():\n",
    "            df[key] = value  # Add hyperparameters as columns\n",
    "        \n",
    "        # Calculate and append mean metrics\n",
    "        mean_row = df.mean(numeric_only=True).to_dict()  # Calculate numeric means\n",
    "        mean_row[\"Fold\"] = \"Mean\"\n",
    "        df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "        # Reorder columns to make \"Fold\" the first column\n",
    "        df = reorder_columns(df)\n",
    "        return df\n",
    "\n",
    "    # Create DataFrames for train, validation, and test metrics\n",
    "    train_df = create_df_with_mean(train_metrics, params)\n",
    "    val_df = create_df_with_mean(val_metrics, params)\n",
    "    test_df = create_df_with_mean(test_metrics, params)  # Single test set treated as one fold\n",
    "\n",
    "    # Save results into an Excel file with separate sheets\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"{file_name}_{current_time}.xlsx\"\n",
    "    with pd.ExcelWriter(results_path) as writer:\n",
    "        train_df.to_excel(writer, index=False, sheet_name=\"Train Results\")\n",
    "        val_df.to_excel(writer, index=False, sheet_name=\"Validation Results\")\n",
    "        test_df.to_excel(writer, index=False, sheet_name=\"Test Results\")\n",
    "\n",
    "    # Apply bold formatting to the \"Mean\" row\n",
    "    def bold_mean_row(sheet_name, df):\n",
    "        wb = load_workbook(results_path)\n",
    "        sheet = wb[sheet_name]\n",
    "        mean_row_index = len(df) + 1  # Excel rows are 1-indexed\n",
    "        for col in range(1, len(df.columns) + 1):  # Loop through all columns\n",
    "            cell = sheet.cell(row=mean_row_index, column=col)\n",
    "            cell.font = Font(bold=True)\n",
    "        wb.save(results_path)\n",
    "\n",
    "    # Apply bold formatting for each sheet\n",
    "    bold_mean_row(\"Train Results\", train_df)\n",
    "    bold_mean_row(\"Validation Results\", val_df)\n",
    "    bold_mean_row(\"Test Results\", test_df)\n",
    "\n",
    "    print(f\"Results saved to {results_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_average_auc(fileName, train_aucs_all, val_aucs_all):\n",
    "    # Convert lists of lists to numpy arrays for easier manipulation\n",
    "    train_aucs_all = np.array(train_aucs_all)\n",
    "    val_aucs_all = np.array(val_aucs_all)\n",
    "    \n",
    "    # Compute mean and standard deviation across folds for each epoch\n",
    "    mean_train_aucs = np.mean(train_aucs_all, axis=0)\n",
    "    # std_train_aucs = np.std(train_aucs_all, axis=0)\n",
    "\n",
    "    mean_val_aucs = np.mean(val_aucs_all, axis=0)\n",
    "    # std_val_aucs = np.std(val_aucs_all, axis=0)\n",
    "\n",
    "    epochs = range(1, len(mean_train_aucs) + 1)\n",
    "\n",
    "    # Plot the average AUC curves with shaded areas for standard deviation\n",
    "    plt.plot(epochs, mean_train_aucs, label='Average Train AUC', color='blue')\n",
    "    # plt.fill_between(epochs, mean_train_aucs - std_train_aucs, mean_train_aucs + std_train_aucs, color='blue', alpha=0.2)\n",
    "\n",
    "    plt.plot(epochs, mean_val_aucs, label='Average Validation AUC', color='orange')\n",
    "    # plt.fill_between(epochs, mean_val_aucs - std_val_aucs, mean_val_aucs + std_val_aucs, color='orange', alpha=0.2)\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Average AUC Curves Across Folds')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./results/AverageAUC_{fileName}_{current_time}.png')\n",
    "    plt.close()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f964cc",
   "metadata": {},
   "source": [
    "## Different GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "79fb47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.3):\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        # Define 3 GATConv layers with residual connections and batch normalization\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.bn1 = BatchNorm(hidden_channels * heads)\n",
    "        self.res_proj1 = nn.Linear(in_channels, hidden_channels * heads)\n",
    "        \n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.bn2 = BatchNorm(hidden_channels * heads)\n",
    "        \n",
    "        self.conv3 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.bn3 = BatchNorm(out_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First layer with residual connection\n",
    "        res1 = x\n",
    "        res1 = self.res_proj1(res1)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x).relu()\n",
    "        x = self.dropout(x) + res1\n",
    "\n",
    "        # Second layer with residual connection\n",
    "        res2 = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x).relu()\n",
    "        x = self.dropout(x) + res2\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        # First GCN layer with residual connection\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        self.res_proj1 = nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = BatchNorm(hidden_channels)\n",
    "        \n",
    "        # Third GCN layer with residual connection\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn3 = BatchNorm(hidden_channels)\n",
    "        self.res_proj3 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final GCN layer to map to output channels\n",
    "        self.conv_out = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn_out = BatchNorm(out_channels)\n",
    "\n",
    "        # Fully connected layers after pooling\n",
    "        self.fc1 = nn.Linear(out_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN layer with residual connection\n",
    "        res1 = self.res_proj1(x)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x).relu() + res1  # Residual connection\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third GCN layer with residual connection\n",
    "        res3 = self.res_proj3(x)\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.bn3(x).relu() + res3  # Residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final GCN layer\n",
    "        x = self.conv_out(x, edge_index).relu()\n",
    "        x = self.bn_out(x).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        \n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.bn2 = BatchNorm(out_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final output with sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GINModel, self).__init__()\n",
    "        \n",
    "        # Define the MLP for GINConv layers\n",
    "        def make_mlp(input_dim, hidden_dim, output_dim):\n",
    "            return Sequential(\n",
    "                Linear(input_dim, hidden_dim),\n",
    "                ReLU(),\n",
    "                Linear(hidden_dim, output_dim),\n",
    "            )\n",
    "\n",
    "        # Define 3 GINConv layers\n",
    "        self.conv1 = GINConv(make_mlp(in_channels, hidden_channels, hidden_channels))\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        \n",
    "        self.conv2 = GINConv(make_mlp(hidden_channels, hidden_channels, out_channels))\n",
    "        self.bn2 = BatchNorm(out_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final output with sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3544d9",
   "metadata": {},
   "source": [
    "## Rum Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2983f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_list, model_name, fileName_base):\n",
    "\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "        \n",
    "    train_data, test_data = train_test_split(data_list, test_size=0.15, random_state=42)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "    \n",
    "    # Define k-fold cross-validation\n",
    "    K = 5  # Number of folds\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define early stopping parameters\n",
    "    PATIENCE = 5\n",
    "\n",
    "    # Store overall best results\n",
    "    best_overall_train_auc = 0.0\n",
    "    best_overall_val_auc = 0.0\n",
    "    best_overall_params = None\n",
    "\n",
    "    # Hyperparameter tuning with k-fold cross-validation\n",
    "    for params in grid:\n",
    "        print(f\"\\nTesting parameters: {params}\")\n",
    "        \n",
    "\n",
    "        fileName = f\"./results/{fileName_base}/lr_{params['lr']}_wd_{params['weight_decay']}_drout_{params['dropout']}_hidChannels{params['hidden_channels']}\"\n",
    "\n",
    "        # Extract the directory path\n",
    "        parent_dir = os.path.dirname(fileName)\n",
    "        if parent_dir and not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "\n",
    "        # Store fold results\n",
    "        fold_train_aucs = []\n",
    "        fold_val_aucs = []\n",
    "\n",
    "        fold_train_metrics = []\n",
    "        fold_val_metrics = []\n",
    "        fold_test_metrics = []\n",
    "\n",
    "\n",
    "        # k-fold cross-validation\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data)):\n",
    "            print(f\"\\nFold {fold + 1}/{K}\")\n",
    "            # Split data into training and test sets for the current fold\n",
    "            fold_train_data = [data_list[i] for i in train_index]\n",
    "            fold_val_data = [data_list[i] for i in val_index]\n",
    "\n",
    "            # Create DataLoader for training and test sets\n",
    "            train_loader = DataLoader(fold_train_data, batch_size=len(fold_train_data), shuffle=False)\n",
    "            val_loader = DataLoader(fold_val_data, batch_size=len(fold_val_data), shuffle=False)\n",
    "            \n",
    "            # Initialize the model with current hyperparameters\n",
    "            model = model_name(in_channels=data_list[0].num_features,\n",
    "                            hidden_channels=params['hidden_channels'],\n",
    "                            out_channels=1,\n",
    "                            dropout=params['dropout'])\n",
    "\n",
    "            # Define loss function and optimizer with L2 regularization\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                        lr=params['lr'],\n",
    "                                        weight_decay=params['weight_decay'])\n",
    "\n",
    "            # Train the GCN model for the current fold\n",
    "            num_epochs = 100\n",
    "\n",
    "            train_metrics = []\n",
    "            val_metrics = []\n",
    "            test_metrics = []\n",
    "            \n",
    "            train_aucs = []\n",
    "            val_aucs = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for data in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss = loss.item()\n",
    "\n",
    "                # Calculate training and validation metrics for each epoch\n",
    "                train_evals = evaluate(train_loader, model)\n",
    "                train_metrics.append(train_evals)\n",
    "                train_aucs.append(train_evals['auc'])\n",
    "\n",
    "                val_evals = evaluate(val_loader, model)\n",
    "                val_metrics.append(val_evals)\n",
    "                val_aucs.append(val_evals['auc'])\n",
    "        \n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "                        f\"Train Acc: {train_evals['accuracy']:.4f}, Val Acc: {val_evals['accuracy']:.4f}\")\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping(val_aucs, PATIENCE):\n",
    "                    print(\"Early stopping triggered...\")\n",
    "                    break\n",
    "            \n",
    "            fold_train_aucs.append(train_aucs)\n",
    "            fold_val_aucs.append(val_aucs)\n",
    "\n",
    "            mean_train_metrics = {key: np.mean([epoch[key] for epoch in train_metrics]) for key in train_metrics[0].keys()}\n",
    "            fold_train_metrics.append(mean_train_metrics)\n",
    "\n",
    "            mean_val_metrics = {key: np.mean([epoch[key] for epoch in val_metrics]) for key in val_metrics[0].keys()}\n",
    "            fold_val_metrics.append(mean_val_metrics)\n",
    "\n",
    "            test_metrics = evaluate(test_loader, model)\n",
    "            fold_test_metrics.append(test_metrics)\n",
    "             \n",
    "        # save evaluation results for each fold for further checking\n",
    "        save_results(fileName, fold_test_metrics, fold_train_metrics, fold_val_metrics, params)\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate average train, validation, and test auc across all folds\n",
    "        avg_train_auc = np.mean([item['auc'] for item in fold_train_metrics])\n",
    "        avg_val_auc = np.mean([item['auc'] for item in fold_val_metrics])\n",
    "        avg_test_auc = np.mean([item['auc'] for item in fold_test_metrics])\n",
    "\n",
    "        print(f\"\\nAverage Train AUC: {avg_train_auc:.4f}\")\n",
    "        print(f\"Average Validation AUC: {avg_val_auc:.4f}\")\n",
    "        print(f\"Average Test AUC: {avg_test_auc:.4f}\")\n",
    "\n",
    "        # Update best overall results\n",
    "        if avg_val_auc > best_overall_val_auc:\n",
    "            best_overall_val_auc = avg_val_auc\n",
    "            best_overall_train_auc = avg_train_auc\n",
    "            best_overall_params = params\n",
    "\n",
    "        \n",
    "    print(f\"\\nBest overall parameters: {best_overall_params}\")\n",
    "    print(f\"Best overall train AUC: {best_overall_train_auc:.4f}\")\n",
    "    print(f\"Best overall val AUC: {best_overall_val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"GATModel\": GATModel,\n",
    "    \"GCNModel\": GCNModel,\n",
    "    \"GraphSAGEModel\": GraphSAGEModel,\n",
    "    \"GINModel\": GINModel\n",
    "}\n",
    "\n",
    "#set these parameters first\n",
    "embedding_sizes=[4, 16, 64]\n",
    "node2vec_folder_base = \".\\\\data\\\\graph_embeddings_\"\n",
    "graph_shapes_file = \".\\\\data\\\\graph_shapes_Yeo_17_mask\"\n",
    "csv_prefix =\"PCMCI\"\n",
    "csv_suffix =\"Yeo-17\"\n",
    "gml_prefix =\"PCMCI\"\n",
    "gml_suffix =\"Yeo-17\"\n",
    "\n",
    "\n",
    "for model_name, model_class in model_dict.items():\n",
    "    for embedding_size in embedding_sizes:\n",
    "        # Adjust the folder path for the current embedding size\n",
    "        node2vec_folder = f\"{node2vec_folder_base}{embedding_size}\"\n",
    "        \n",
    "        print(f\"\\nRunning {model_name} with embedding size {embedding_size}...\")\n",
    "        \n",
    "        # Load the data\n",
    "        data_list = load_data(node2vec_folder, graph_shapes_file, csv_prefix, csv_suffix, gml_prefix, gml_suffix)\n",
    "        \n",
    "        print(f\"Number of loaded samples: {len(data_list)}\")\n",
    "\n",
    "        # If no data was loaded, raise an error\n",
    "        if len(data_list) == 0:\n",
    "            raise ValueError(f\"No data samples were loaded for embedding size {embedding_size}. Check the folder path or data format.\")\n",
    "        \n",
    "\n",
    "        fileName_base = f\"model_{model_class.__name__}_embeddingSize_{embedding_size}\"\n",
    "        # Run the model\n",
    "        run_model(data_list, model_class, fileName_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
