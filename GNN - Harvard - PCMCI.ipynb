{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db20886",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7be9bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded samples: 134\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm, global_mean_pool, global_add_pool, global_max_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Define folder path and TSV file containing labels\n",
    "participants_xlsx_file = '.\\\\data\\\\participants.xsls'\n",
    "participants_tsv_file = '.\\\\data\\\\participants.tsv'\n",
    "node2vec_folder = \".\\\\data\\\\graph_embeddings_4\"\n",
    "graph_shapes_file = \".\\\\data\\\\graph_shapes\"\n",
    "\n",
    "# Load TSV file\n",
    "df_labels = pd.read_csv(participants_tsv_file, sep='\\t')\n",
    "\n",
    "# Filter out rows where 'group' is 'n/a'\n",
    "df_labels = df_labels[df_labels['group'].notna() & (df_labels['group'] != 'n/a')]\n",
    "\n",
    "# Extract labels as a dictionary and convert to integers\n",
    "labels_dict = dict(zip(df_labels['participant_id'], df_labels['group'].astype(int)))\n",
    "\n",
    "# Function to load data\n",
    "def load_data(node2vec_folder, labels_dict):\n",
    "    data_list = []  # List to store loaded data\n",
    "    missing_files = []  # List to track missing files\n",
    "    processed_subs = []  # List to track processed subjects\n",
    "\n",
    "    for sub_id, label in labels_dict.items():\n",
    "        # Correct the format of the subject ID to match the file format\n",
    "        sub_id_corrected = sub_id.replace('sub-', 'Sub')\n",
    "        csv_file = os.path.join(node2vec_folder, f\"Node2Vec_PCMCI_{sub_id_corrected}_Harvard.csv\")\n",
    "        gml_file = os.path.join(graph_shapes_file, f\"GNN input_PCMCI_{sub_id_corrected}_Harvard.gml\")\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(csv_file) and os.path.exists(gml_file):\n",
    "            # Load node features from CSV\n",
    "            node_features = pd.read_csv(csv_file, header=0, index_col=0).values\n",
    "            num_nodes, num_features = node_features.shape\n",
    "\n",
    "            # Load the graph structure from GML file\n",
    "            nx_graph = nx.read_gml(gml_file)\n",
    "            data = from_networkx(nx_graph)\n",
    "            edge_index = data.edge_index\n",
    "\n",
    "            # Create a complete graph for edge_index\n",
    "            # edge_index = torch.combinations(torch.arange(num_nodes), 2).t()\n",
    "            \n",
    "            # Convert data to PyTorch tensors\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "            y = torch.tensor([label - 1], dtype=torch.long)  # Convert labels to 0 and 1\n",
    "            \n",
    "            # Create a Data object for PyTorch Geometric\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "            processed_subs.append(sub_id)\n",
    "        else:\n",
    "            missing_files.append(sub_id)  # Add missing subject ID to the list\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Load the data\n",
    "data_list = load_data(node2vec_folder, labels_dict)\n",
    "\n",
    "# Check the number of loaded data samples\n",
    "print(f\"Number of loaded samples: {len(data_list)}\")\n",
    "\n",
    "# If no data was loaded, raise an error\n",
    "if len(data_list) == 0:\n",
    "    raise ValueError(\"No data samples were loaded. Please check the file path and format.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd325b",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "db69403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for early stopping\n",
    "def early_stopping(val_accuracies, patience):\n",
    "    if len(val_accuracies) > patience:\n",
    "        recent_accuracies = val_accuracies[-patience:]\n",
    "        if max(recent_accuracies) <= val_accuracies[-patience-1]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e2fd6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3914a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter search with Grid Search\n",
    "param_grid = {\n",
    "    'hidden_channels': [16, 32, 64],\n",
    "    'dropout': [0.2, 0.4],\n",
    "    'lr': [0.01, 0.0001],\n",
    "    'weight_decay': [1e-2, 1e-4]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94cd7f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84dabfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, mean_squared_error, precision_recall_curve, auc as sklearn_auc\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    \n",
    "    # print(labels)\n",
    "    # print(preds)\n",
    "    # print(preds_binary)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds_binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds_binary, average='binary', zero_division=0)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)  # MSE computation\n",
    "    conf_matrix = confusion_matrix(labels, preds_binary)\n",
    "    average_precision = average_precision_score(labels, preds)\n",
    "    # tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    # specificity = tn / (tn + fp)\n",
    "\n",
    "    # Compute precision-recall curve\n",
    "    # precision_curve, recall_curve, _ = precision_recall_curve(labels, preds)\n",
    "    # auprc = sklearn_auc(recall_curve, precision_curve)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"specificity\": specificity,\n",
    "        \"conf_matrix\": conf_matrix,\n",
    "        \"ap\": average_precision,\n",
    "        \"mse\": mse,\n",
    "        # \"auprc\":auprc\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluation function to calculate accuracy\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            res = compute_metrics(out, data.y)\n",
    "            \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "646ecd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def save_Results(fileName, test_metrics):\n",
    "\n",
    "    with open(f'./results/test_metrics_{fileName}_{current_time}.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow([\"Metric\", \"Value\"])\n",
    "        \n",
    "\n",
    "        # Write each metric and its value\n",
    "        for metric, value in test_metrics.items():\n",
    "            writer.writerow([metric, value])\n",
    "\n",
    "\n",
    "def plot_average_auc(fileName, train_aucs_all, val_aucs_all):\n",
    "    # Convert lists of lists to numpy arrays for easier manipulation\n",
    "    train_aucs_all = np.array(train_aucs_all)\n",
    "    val_aucs_all = np.array(val_aucs_all)\n",
    "    \n",
    "    # Compute mean and standard deviation across folds for each epoch\n",
    "    mean_train_aucs = np.mean(train_aucs_all, axis=0)\n",
    "    # std_train_aucs = np.std(train_aucs_all, axis=0)\n",
    "\n",
    "    mean_val_aucs = np.mean(val_aucs_all, axis=0)\n",
    "    # std_val_aucs = np.std(val_aucs_all, axis=0)\n",
    "\n",
    "    epochs = range(1, len(mean_train_aucs) + 1)\n",
    "\n",
    "    # Plot the average AUC curves with shaded areas for standard deviation\n",
    "    plt.plot(epochs, mean_train_aucs, label='Average Train AUC', color='blue')\n",
    "    # plt.fill_between(epochs, mean_train_aucs - std_train_aucs, mean_train_aucs + std_train_aucs, color='blue', alpha=0.2)\n",
    "\n",
    "    plt.plot(epochs, mean_val_aucs, label='Average Validation AUC', color='orange')\n",
    "    # plt.fill_between(epochs, mean_val_aucs - std_val_aucs, mean_val_aucs + std_val_aucs, color='orange', alpha=0.2)\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Average AUC Curves Across Folds')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./results/AverageAUC_{fileName}_{current_time}.png')\n",
    "    plt.close()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f964cc",
   "metadata": {},
   "source": [
    "## Different GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79fb47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.3):\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        # Define 3 GATConv layers with residual connections and batch normalization\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.bn1 = BatchNorm(hidden_channels * heads)\n",
    "        self.res_proj1 = nn.Linear(in_channels, hidden_channels * heads)\n",
    "        \n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.bn2 = BatchNorm(hidden_channels * heads)\n",
    "        \n",
    "        self.conv3 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.bn3 = BatchNorm(out_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First layer with residual connection\n",
    "        res1 = x\n",
    "        res1 = self.res_proj1(res1)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x).relu()\n",
    "        x = self.dropout(x) + res1\n",
    "\n",
    "        # Second layer with residual connection\n",
    "        res2 = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x).relu()\n",
    "        x = self.dropout(x) + res2\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        # First GCN layer with residual connection\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        self.res_proj1 = nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = BatchNorm(hidden_channels)\n",
    "        \n",
    "        # Third GCN layer with residual connection\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn3 = BatchNorm(hidden_channels)\n",
    "        self.res_proj3 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final GCN layer to map to output channels\n",
    "        self.conv_out = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn_out = BatchNorm(out_channels)\n",
    "\n",
    "        # Fully connected layers after pooling\n",
    "        self.fc1 = nn.Linear(out_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN layer with residual connection\n",
    "        res1 = self.res_proj1(x)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x).relu() + res1  # Residual connection\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third GCN layer with residual connection\n",
    "        res3 = self.res_proj3(x)\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.bn3(x).relu() + res3  # Residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final GCN layer\n",
    "        x = self.conv_out(x, edge_index).relu()\n",
    "        x = self.bn_out(x).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3544d9",
   "metadata": {},
   "source": [
    "## Rum Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2983f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name):\n",
    "\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "        \n",
    "    train_data, test_data = train_test_split(data_list, test_size=0.1, random_state=42)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "    \n",
    "    # Define k-fold cross-validation\n",
    "    K = 5  # Number of folds\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define early stopping parameters\n",
    "    PATIENCE = 5\n",
    "\n",
    "    # Store overall best results\n",
    "    best_overall_train_auc = 0.0\n",
    "    best_overall_test_auc = 0.0\n",
    "    best_overall_params = None\n",
    "\n",
    "    # Hyperparameter tuning with k-fold cross-validation\n",
    "    for params in grid:\n",
    "        print(f\"\\nTesting parameters: {params}\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_train_aucs = []\n",
    "        fold_val_aucs = []\n",
    "\n",
    "        # k-fold cross-validation\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_data)):\n",
    "            print(f\"\\nFold {fold + 1}/{K}\")\n",
    "            # Split data into training and test sets for the current fold\n",
    "            fold_train_data = [data_list[i] for i in train_index]\n",
    "            fold_val_data = [data_list[i] for i in val_index]\n",
    "\n",
    "            # print('/////fold_train_data')\n",
    "            # print(fold_train_data)\n",
    "            # print(fold_train_data[0])\n",
    "            # print(len(fold_train_data))\n",
    "\n",
    "            # Create DataLoader for training and test sets\n",
    "            train_loader = DataLoader(fold_train_data, batch_size=len(fold_train_data), shuffle=False)\n",
    "            val_loader = DataLoader(fold_val_data, batch_size=len(fold_val_data), shuffle=False)\n",
    "            \n",
    "            # Initialize the model with current hyperparameters\n",
    "            model = model_name(in_channels=data_list[0].num_features,\n",
    "                            hidden_channels=params['hidden_channels'],\n",
    "                            out_channels=1,\n",
    "                            dropout=params['dropout'])\n",
    "\n",
    "            # Define loss function and optimizer with L2 regularization\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                        lr=params['lr'],\n",
    "                                        weight_decay=params['weight_decay'])\n",
    "\n",
    "            # Train the GCN model for the current fold\n",
    "            num_epochs = 100\n",
    "            train_aucs = []\n",
    "            val_aucs = []\n",
    "\n",
    "            \n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for data in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss = loss.item()\n",
    "\n",
    "                # Calculate training and validation metrics for each epoch\n",
    "                train_metrics = evaluate(train_loader, model)\n",
    "                train_aucs.append(train_metrics['auc'])\n",
    "\n",
    "                val_metrics = evaluate(val_loader, model)\n",
    "                val_aucs.append(val_metrics['auc'])\n",
    "        \n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "                        f\"Train Acc: {train_metrics['accuracy']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping(val_aucs, PATIENCE):\n",
    "                    print(\"Early stopping triggered...\")\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            fold_train_aucs.append(train_aucs)\n",
    "            fold_val_aucs.append(val_aucs)\n",
    "\n",
    "        fileName = f\"lr_{params['lr']}_wd_{params['weight_decay']}_drout_{params['dropout']}_hidChannels{params['hidden_channels']}\"\n",
    "        # plot_average_auc(fileName, fold_train_aucs, fold_val_aucs)\n",
    "\n",
    "        # Calculate average train and val auc across all folds\n",
    "        avg_train_auc = np.mean([item for sublist in fold_train_aucs for item in sublist])\n",
    "        avg_val_auc = np.mean([item for sublist in fold_val_aucs for item in sublist])\n",
    "\n",
    "        print(f\"\\nAverage Train AUC with current parameters: {avg_train_auc:.4f}\")\n",
    "        print(f\"Average Validation AUC with current parameters: {avg_val_auc:.4f}\")\n",
    "\n",
    "        # Update best overall results\n",
    "        if avg_val_auc > best_overall_test_auc:\n",
    "            best_overall_test_auc = avg_val_auc\n",
    "            best_overall_train_auc = avg_train_auc\n",
    "            best_overall_params = params\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # ///////////\n",
    "        # Calculate final train and test metrics for the current fold\n",
    "        final_test_metrics = evaluate(test_loader, model)\n",
    "        save_Results(fileName, final_test_metrics)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\nBest overall parameters: {best_overall_params}\")\n",
    "    print(f\"Best overall train accuracy: {best_overall_train_auc:.4f}\")\n",
    "    print(f\"Best overall test accuracy: {best_overall_test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"GATModel\": GATModel,\n",
    "    \"GCNModel\": GCNModel\n",
    "}\n",
    "\n",
    "# Prompt the user for the model name\n",
    "model_name = 'GATModel'\n",
    "\n",
    "# Get the model class from the dictionary based on the input\n",
    "model_class = model_dict.get(model_name)\n",
    "\n",
    "if model_class is None:\n",
    "    print(f\"Model '{model_name}' not found. Please enter a valid model name.\")\n",
    "else:\n",
    "    run_model(model_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
